import cv2
import sys
import os
import numpy as np
import random

# unit box is 120x120
UnitSpacing = 120
ImgX = 1920
ImgY = 1080
Scales=[1, 1, 2, 2]
AspectRatios=[1, 2, 1, 2]
SkipFrames = 10
MinThreshold = 0.3

# read all of the args. args should be filenames of the ground truth
# data generated by ROIdemo.py.  These filenames must be numpy arrays
# and named with the format vidfilename_objname.npy
VideoGroundTruth = {}
for arg in sys.argv[1:]:
	gtarr = np.load(arg, allow_pickle=True)
	gttup = (int(gtarr[0]),int(gtarr[1]),int(gtarr[2]),int(gtarr[3]))
	fileName = os.path.split(arg[:arg.find('_')])[1]
	if fileName in VideoGroundTruth:
		VideoGroundTruth[fileName].append(gttup)
	else:
		VideoGroundTruth[fileName] = [gttup]
	img = cv2.imread("Output/FirstFrame/"+fileName+".jpg",cv2.IMREAD_GRAYSCALE)
	#cv2.rectangle(img,
	#cv2.imshow(fileName,img)
	#k = cv2.waitKey(100)


# Each video frame is 1920x1080 so I'm trying to break up each frame
# So I can have as many non-overlapping boxes that are 128x128 as
# possible.
# 1920/128 = 15
# 1080/128 ~= 8 so there will be some gaps. I'm ok with that.

# Generate all anchor points
def GenerateAnchorPoints():
	# calculate anchor point spacing along x
	nx = int(ImgX/UnitSpacing)
	# calculate anchor point spacing along y
	ny = int(ImgY/UnitSpacing)
	print("nx="+str(nx))
	print("ny="+str(ny))
	# calculate all of the X and Y anchor points
	apxs = np.array(range(0,nx))*(ImgX/nx)+UnitSpacing/2
	apys = np.array(range(0,ny))*(ImgY/ny)+UnitSpacing/2
	# combine all of the x/y combinations into a list
	aps=[]
	for apx in apxs:
		for apy in apys:
			tup = (int(apx),int(apy))
			aps.append(tup)
	return aps

# Generate boxes given the anchor points and
# a given scale and aspect ratio
def GenerateBoxes(anchors, scale, aspectRatio):
	boxes = []
	dx = (scale * UnitSpacing)
	dy = (dx * aspectRatio)
	for ap in anchors:
		# "center" of the box
		cx = min(max(dx,ap[0]),ImgX-dx)
		cy = min(max(dy,ap[1]),ImgY-dy)
		ux = cx-dx/2
		uy = cy-dy/2
		tup=(int(ux),int(uy),int(dx),int(dy))
		boxes.append(tup)
	return boxes

def CalcOverlap(face, box):
	# extract face coords
	fx1 = face[0]
	fx2 = face[0]+face[2]
	fy1 = face[1]
	fy2 = face[1]+face[3]
	# calc area of the face
	fa = face[2]*face[3]

	# extract box coords
	bx1 = box[0]
	bx2 = box[0]+box[2]
	by1 = box[1]
	by2 = box[1]+box[3]
	# calc area of the box
	ba = box[2]*box[3]

	# calc area of the intersection
	ia = max(0,min(fx2,bx2)-max(fx1,bx1))*max(0,min(fy2,by2)-max(fy1,by1))
	# calc IoU
	ratio = float(ia)/float(ba+fa-ia)
	return ratio

def BuildTrainBoxes(faces, boxes):
	boxes = list(boxes) # copy the boxes list
	faceBoxes = []
	# for each face...
	for face in faces:
		bestTh = -1
		bestidx = -1
		idx = 0
		# for each box
		for box in boxes:
			# calculate the IoU
			th = CalcOverlap(face, box)
			# keep the best one
			if th > bestTh:
				bestTh = th
				bestidx = idx
			idx = idx + 1

		# if we found a good box and it meets the minimum threshold
		if bestTh > MinThreshold:
			print("best threshold: " + str(bestTh))
			# keep the best box which matches the face
			faceBoxes.append(boxes[bestidx])
			# remove the box from boxes list
			boxes.remove(boxes[bestidx])

	return faceBoxes, boxes

def ExtractBox(img, box):
	sub = img[box[1]:box[1]+box[3],box[0]:box[0]+box[2]]
	return sub

print("Calculating total frames...")
totalFrames = 0
for videoName in VideoGroundTruth:
	cap = cv2.VideoCapture("videos/BackCameraClips/" + videoName + ".mp4")
	totalFrames = totalFrames +  int((cap.get(cv2.CAP_PROP_FRAME_COUNT)+SkipFrames-1)/SkipFrames)

print("Using " + str(totalFrames) + " frames.")

aps = GenerateAnchorPoints()
for scale, aspectRatio in zip(Scales, AspectRatios):
	height = UnitSpacing*scale;
	width = height*aspectRatio
	boxShape = (width, height)
	#
	# Estimate the number of data items as totalFrames * 6
	y = np.zeros((totalFrames*6,))
	X = np.zeros((totalFrames*6,boxShape[0],boxShape[1]))
	dataNo = 0
	boxes = GenerateBoxes(aps, scale, aspectRatio)
	for videoName in VideoGroundTruth:
		vf = 0
		cap = cv2.VideoCapture("videos/BackCameraClips/" + videoName + ".mp4")
		faces = VideoGroundTruth[videoName]
		faceBoxes, otherBoxes = BuildTrainBoxes(faces,boxes)
		while True:
			ret, frame = cap.read()
			if ret == False:
				break

			# Convert to grayscale
			gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

			# Extract faceboxes data
			for face in faceBoxes:
				data = ExtractBox(gray, face)
				y[dataNo] = 1
				X[dataNo,:width,:height] = data
				dataNo = dataNo + 1

			# shuffle the other boxes so we always grab random boxes
			random.shuffle(otherBoxes)

			# Extract other boxes data
			# get the same number as faces
			for i in range(0,len(faceBoxes)):
				data = ExtractBox(gray, otherBoxes[i])
				y[dataNo] = 0
				X[dataNo,:width,:height] = data
				dataNo = dataNo + 1

			if vf == 0:
				# draw the original GT face boxes
				for face in faces:
					print(face)
					cv2.rectangle(gray,(face[0],face[1]),(face[0]+face[2],face[1]+face[3]),(128,0,0),3)
				# draw the best matching face boxes
				for face in faceBoxes:
					print(face)
					cv2.rectangle(gray,(face[0],face[1]),(face[0]+face[2],face[1]+face[3]),(0,0,0),3)
				# draw the randomly selected boxes
				for i in range(0,len(faceBoxes)):
					ob = otherBoxes[i]
					print(ob)
					cv2.rectangle(gray,(ob[0],ob[1]),(ob[0]+ob[2],ob[1]+ob[3]),(255,0,0),3)
				#cv2.imshow("frame 0", gray)
				#cv2.waitKey(0)
				#cv2.destroyAllWindows()

			# skip some frames
			for i in range(1,SkipFrames):
				ret, frame = cap.read()
			vf = vf + 1

	y = np.resize(y,(dataNo,))
	X = np.resize(X,(dataNo,boxShape[0],boxShape[1]))

	np.save("y_{}_{}.npy".format(scale,aspectRatio), y)
	np.save("X_{}_{}.npy".format(scale,aspectRatio), X)
